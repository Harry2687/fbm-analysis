---
title: "Latent Dirichlet Allocation"
format: html
---

```{python}
import json
import os
import time
import pandas as pd
import numpy as np
import datetime as dt
import matplotlib as plt
import gensim as gs
```

```{python}
import functions.fbmessenger as fbm
```

Import and clean data.

```{python}
chat_data = fbm.ms_import_data('data/the_office')

# standard cleaning stuff
chat_data['clean_content'] = (
    chat_data['content']
    .str.lower()
    .str.strip()
    .str.replace('[^a-z\\s]', '', regex=True)
    .str.replace('\\s{2,}', ' ', regex=True)
)

# specify which messages are actually chat actions which aren't part of the conversation
chat_actions = [
    'reacted to your message'
]

# remove chat actions
chat_data = chat_data[
    ~chat_data['clean_content']
    .str.contains(
        '|'.join(chat_actions)
    )
]

# define stopwords which are not included in gensim
custom_stopwords = [
    'u', 
    'lmao', 
    'lol', 
    'ur', 
    'like', 
    'yea', 
    'thats', 
    'nah', 
    'im', 
    'yeh', 
    'dont',
    'yeah', 
    'gonna', 
    'didnt',
    'idk',
    'got',
    'r',
    'sure',
    'come',
    'stuff'
    'k'
]

# remove gensim and custom stopwords
chat_data['clean_content_rmstopwords'] = (
    chat_data['clean_content']
    .apply(gs.parsing.preprocessing.remove_stopwords)
    .apply(fbm.remove_custom_stopwords, args=(custom_stopwords,))
    .str.strip()
    .str.replace('\\s{2,}', ' ', regex=True)
)
```

Count number of occurances of specified word by sender.

```{python}
chat_data['clean_content_splitlist'] = (
    chat_data['clean_content']
    .str.split(' ')
)

search_word = 'nah'

chat_data_wcount = (
    chat_data[['sender_name', 'clean_content_splitlist']]
    .explode('clean_content_splitlist')
    .query('clean_content_splitlist == @search_word')
    .value_counts()
    .reset_index()
    .drop('clean_content_splitlist', axis=1)
    .sort_values('count', ascending=False)
)

chat_data_wcount
```

Split messages into conversations which are separated by at least 10 minutes.

```{python}
# calculate difference between each message
chat_data['time_diff'] = (
    chat_data['timestamp']
    .diff()
    .fillna(pd.Timedelta(seconds=0))
)
chat_data['time_diff'] = (
    chat_data['time_diff']
    .dt.total_seconds()
)

# specify how long until message group becomes a new conversation
conv_cutoff = 600

# group dataframe into different conversations based on cutoff value
chat_data['new_conv'] = chat_data['time_diff'] > conv_cutoff
chat_data['conv_num'] = 'Conv ' + (
    chat_data['new_conv']
    .cumsum()
    .astype(str)
)

# join together messages in the same coversation
conversations = (
    chat_data
    .groupby('conv_num')
    ['clean_content_rmstopwords']
    .apply(lambda x: ' '.join(map(str, x)))
    .str.strip()
    .str.replace('\\s{2,}', ' ', regex=True)
    .reset_index()
)
```

Run LDA, where documents are the previously defined conversations.

```{python}
documents = (
    conversations['clean_content_rmstopwords']
    .tolist()
)
texts = [doc.split() for doc in documents]
dictionary = gs.corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]
ldamodel = gs.models.ldamodel.LdaModel(
    corpus,
    num_topics=5,
    id2word=dictionary,
    passes=20
)
for topic in ldamodel.print_topics(num_topics=10, num_words=10):
    print(topic)
```

```{python}
ldamodel_coherence = gs.models.CoherenceModel(
    model=ldamodel,
    texts=texts,
    dictionary=dictionary,
    coherence='u_mass'
)

ldamodel_coherence.get_coherence()
```

Hyperparameter tuning

```{python}
n_topics_values = np.arange(2, 11, 1)

tuning_results = pd.DataFrame(
    columns=[
        'n_topics',
        'umass_coherence'
    ]
)

start_time = time.time()
for index in range(len(n_topics_values)):
    n_topics = n_topics_values[index]

    ldamodel = gs.models.ldamodel.LdaModel(
        corpus=corpus,
        num_topics=n_topics,
        id2word=dictionary,
        passes=20
    )

    ldamodel_coherence = gs.models.CoherenceModel(
        model=ldamodel,
        texts=texts,
        dictionary=dictionary,
        coherence='u_mass'
    )

    umass_coherence = ldamodel_coherence.get_coherence()

    tuning_results.loc[index] = (
        [n_topics] + 
        [umass_coherence]
    )
end_time = time.time()
execution_time = end_time - start_time

tuning_results = tuning_results.sort_values('umass_coherence', ascending=False)

tuning_results
```