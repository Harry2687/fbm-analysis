---
title: "Retrieval Augmented Generation"
format: html
---

# Convert json files to single txt file.

```{python}
import modules.preprocessing as prep

chat_data = prep.ms_import_data('data/the_office')

# standard cleaning stuff
chat_data['clean_content'] = (
    chat_data['content']
    .str.strip()
    .str.replace('[^a-zA-Z0-9\\s]', '', regex=True)
    .str.replace('\\s{2,}', ' ', regex=True)
)

# export with timestamps
chat_data['conv_text'] = (
    chat_data['sender_name'] + 
    ' at ' + 
    chat_data['timestamp'].dt.strftime('%Y-%m-%d %H:%M:%S') + 
    ': ' + 
    chat_data['clean_content']
)
chat_content = chat_data[['conv_text']]
chat_content.to_csv('source_documents/chatlog_w_timestamp.txt', index=False, header=False)

# export without timestamps
chat_data['conv_text'] = (
    chat_data['sender_name'] + 
    ': ' + 
    chat_data['clean_content']
)
chat_content = chat_data[['conv_text']]
chat_content.to_csv('source_documents/chatlog.txt', index=False, header=False)
```

# Test RAG with different settings

Settings to test:
- Chunk size
    - 1000, 750, 500, 250
- Chunk overlap (set this to 10% of chunk size)
    - 100, 75, 50, 25
- Number of chunks given as context
- LLM model

Vector databases need to be set up for the first 2 variables

```{python}
from modules.models import ChatFBM
from modules.rag_demo import rag_demo
import os
```

## Create vector databases for different chunk sizes

```{python}
chunk_sizes = [1000, 750, 500, 250]
for size in chunk_sizes:
    model = ChatFBM()
    db_path = f'databases/chatlog_vectordb_chunksize{size}'
    overlap = int(0.1*size)
    
    if not os.path.exists(db_path):
        model.ingest(
            file_path='source_documents/chatlog.txt',
            db_path=db_path,
            chunk_size=size,
            chunk_overlap=overlap
        )
```

## Check number of chunks for each chunk size

```{python}
chunk_sizes = [1000, 750, 500, 250]
for size in chunk_sizes:
    db_path = f'databases/chatlog_vectordb_chunksize{size}'
    
    if os.path.exists(db_path):
        model = ChatFBM()
        model.load_db(db_path)
        n_chunks = len(model.vector_store.get()['documents'])
        print(f'Number of chunks for chunk size {size}: {n_chunks}')
```

## Test queries for each chunk size

```{python}
chunk_sizes = [1000, 750, 500, 250]
queries = [
    "What games does Harry play?",
    "What topics does Harry talk about with Dhruv?",
    "What topics does Harry talk about with Mansoor?",
    "Who would you consider to be the funniest?",
    "Who would you consider to be the smartest?"
]

for size in chunk_sizes:
    context_size = round(5000/size)
    db_path = f'databases/chatlog_vectordb_chunksize{size}'
    query = 'Would you consider Mansoor to be haram?'

    print(f'Chunk size {size}, with {context_size} chunks for context')
    for query in queries:
        print(f'Query: {query}')
        rag_demo(
            query=query,
            db_file_path=db_path,
            context_size=context_size,
            print_docs=False
        )
        print('\n')
```