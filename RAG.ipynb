{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Retrieval Augmented Generation\"\n",
        "format: html\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Convert json files to single txt file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import modules.preprocessing as prep\n",
        "prep.convert_to_txt('data/the_office', 'source_documents/the_office.txt')\n",
        "prep.convert_to_txt('data/fuck_harry_zhing', 'source_documents/fuck_harry_zhing.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test RAG with different settings\n",
        "\n",
        "Settings to test:\n",
        "- Chunk size\n",
        "    - 1000, 750, 500, 250\n",
        "- Chunk overlap (set this to 10% of chunk size)\n",
        "    - 100, 75, 50, 25\n",
        "- Number of chunks given as context\n",
        "- LLM model\n",
        "\n",
        "Vector databases need to be set up for the first 2 variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from modules.rag.models import ChatFBM\n",
        "from modules.rag.rag_demo import rag_demo\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create vector databases for different chunk sizes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "chunk_sizes = [1000, 750, 500, 250]\n",
        "chat = 'the_office'\n",
        "for size in chunk_sizes:\n",
        "    model = ChatFBM()\n",
        "    db_path = f'databases/{chat}_vectordb_chunksize{size}'\n",
        "    overlap = int(0.1*size)\n",
        "    \n",
        "    if not os.path.exists(db_path):\n",
        "        model.ingest(\n",
        "            file_path=f'source_documents/{chat}.txt',\n",
        "            db_path=db_path,\n",
        "            chunk_size=size,\n",
        "            chunk_overlap=overlap\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Check number of chunks for each chunk size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "chunk_sizes = [1000, 750, 500, 250]\n",
        "chat = 'the_office'\n",
        "for size in chunk_sizes:\n",
        "    db_path = f'databases/{chat}_vectordb_chunksize{size}'\n",
        "    \n",
        "    if os.path.exists(db_path):\n",
        "        model = ChatFBM()\n",
        "        model.load_db(db_path)\n",
        "        n_chunks = len(model.vector_store.get()['documents'])\n",
        "        print(f'Number of chunks in {chat} for chunk size {size}: {n_chunks}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test queries for each chunk size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "chunk_sizes = [500]\n",
        "chat_name = 'the_office'\n",
        "queries = [\n",
        "    \"What games does Harry play?\",\n",
        "    \"What topics does Harry talk about with Dhruv?\",\n",
        "    \"What topics does Harry talk about with Mansoor?\",\n",
        "    \"What is the overall sentiment of messages sent by Harry?\",\n",
        "    \"Which topic has generated the most positive sentiment?\",\n",
        "    \"Which topic has generated the most negative sentiment\"\n",
        "]\n",
        "\n",
        "for size in chunk_sizes:\n",
        "    context_size = round(5000/size)\n",
        "    db_path = f'databases/{chat_name}_vectordb_chunksize{size}'\n",
        "\n",
        "    print(f'Chunk size {size}, with {context_size} chunks for context')\n",
        "    for query in queries:\n",
        "        print(f'Query: {query}')\n",
        "        rag_demo(\n",
        "            query=query,\n",
        "            db_file_path=db_path,\n",
        "            context_size=context_size,\n",
        "            print_docs=True,\n",
        "            model_name='llama3.1:8b'\n",
        "        )\n",
        "        print('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}